{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating Summarization of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store extracted data\n",
    "scraped_data = []\n",
    "\n",
    "class LinksSpider(scrapy.Spider):\n",
    "    name = \"link_spider\"\n",
    "    start_urls = [\"https://www.langchain.com\"]  # Replace with your target link\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract all links from the main page\n",
    "        links = response.css(\"a::attr(href)\").getall()\n",
    "        for link in links:\n",
    "            if link.startswith(\"http\"):  # Ensure it's a valid absolute URL\n",
    "                yield response.follow(link, callback=self.parse_link)\n",
    "\n",
    "    def parse_link(self, response):\n",
    "        # Extract text content from the page\n",
    "        text_content = \" \".join(response.css(\"p::text\").getall())  # Extract all paragraph text\n",
    "\n",
    "        if text_content:\n",
    "            # Generate a summary (max length 100 words)\n",
    "            summary = summarizer(text_content, max_length=100, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "        else:\n",
    "            summary = \"No summary available\"\n",
    "\n",
    "        # Store extracted data in a list\n",
    "        scraped_data.append({\"url\": response.url,\n",
    "            \"title\": response.css(\"title::text\").get(),\"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 12:23:19 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2025-02-11 12:23:19 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Windows-11-10.0.22631-SP0\n",
      "2025-02-11 12:23:19 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-02-11 12:23:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2025-02-11 12:23:19 [scrapy.extensions.telnet] INFO: Telnet Password: 2606bb6bba98daed\n",
      "2025-02-11 12:23:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-02-11 12:23:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2025-02-11 12:23:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-02-11 12:23:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-02-11 12:23:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-02-11 12:23:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-02-11 12:23:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.langchain.com> (referer: None)\n",
      "2025-02-11 12:23:19 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://docs.smith.langchain.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.langchain.dev/llms-accelerate-adyens-support-team-through-smart-ticket-routing-and-support-agent-copilot/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.langchain.dev/ally-financial-collaborates-with-langchain-to-deliver-critical-coding-module-to-mask-personal-identifying-information-in-a-compliant-and-safe-manner/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/> (referer: https://www.langchain.com)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://interrupt.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://langchain-ai.github.io/langgraph/tutorials/introduction/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://status.smith.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/langchain-ai> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (308) to <GET https://python.langchain.com/docs/introduction/> from <GET https://python.langchain.com/>\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://python.langchain.com/docs/integrations/providers/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://trust.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (999) <GET https://www.linkedin.com/company/langchain/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:23:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://smith.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:24:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:24:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://docs.smith.langchain.com/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n",
      "Your max_length is set to 100, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 100, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "2025-02-11 12:25:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <999 https://www.linkedin.com/company/langchain/>: HTTP status code is not handled or not allowed\n",
      "2025-02-11 12:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://python.langchain.com/docs/introduction/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://x.com/LangChainAI> from <GET https://twitter.com/LangChainAI>\n",
      "2025-02-11 12:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (308) to <GET https://js.langchain.com/docs/introduction/> from <GET https://js.langchain.com/docs/get_started/introduction/>\n",
      "2025-02-11 12:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://js.langchain.com/docs/introduction/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:02 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-02-11 12:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.youtube.com/@LangChain> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.langchain.dev/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://academy.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://x.com/LangChainAI> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n",
      "2025-02-11 12:25:27 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 3 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-02-11 12:25:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.smith.langchain.com/> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://changelog.langchain.com/> (referer: https://www.langchain.com)\n",
      "Your max_length is set to 100, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "2025-02-11 12:25:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://x.com/LangChainAI> (referer: https://www.langchain.com)\n",
      "2025-02-11 12:25:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-02-11 12:25:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 2,\n",
      " 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,\n",
      " 'downloader/request_bytes': 7606,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 861007,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 21,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'downloader/response_status_count/308': 2,\n",
      " 'downloader/response_status_count/999': 1,\n",
      " 'dupefilter/filtered': 17,\n",
      " 'elapsed_time_seconds': 137.115449,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 2, 11, 6, 55, 36, 501603, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 3529386,\n",
      " 'httpcompression/response_count': 19,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/999': 1,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 29,\n",
      " 'log_count/INFO': 13,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 22,\n",
      " 'responses_per_minute': None,\n",
      " 'retry/count': 2,\n",
      " 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2025, 2, 11, 6, 53, 19, 386154, tzinfo=datetime.timezone.utc)}\n",
      "2025-02-11 12:25:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Run the spider in Jupyter Notebook\n",
    "process = CrawlerProcess()\n",
    "process.crawl(LinksSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting The Documents Infromation To Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists=[]\n",
    "for a in scraped_data:\n",
    "    lists.append(a['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A publicly-traded financial technology platform is helping large companies like Meta, Uber, H&M, and Microsoft achieve their ambitions faster by providing end-to-end payments capabilities, data-driven insights, and financial products in a single global solution. With more merchants signing on and with increased transaction volume comes increased pressure on support teams and a team at Adyen that immediately sought out leveraged solutions.',\n",
       " 'Elastic, a leading search analytics company, serving over 20k customers worldwide, enables organizations to securely harness search-powered AI so anyone can find the answers they need in real-time using all their data, at scale.',\n",
       " 'Ally Financial, the largest digital-only bank in the US and a leading auto lender, has recently collaborated with LangChain to release the first initial coding module that addresses a significant challenge for AI developers working with personal identifiable information (PII) in highly regulated, consumer-focused industries.',\n",
       " 'Create a new folder for the project. Run the code. Add your API keys. Add the constructor to your code. Run your code with the same command.',\n",
       " 'Exploring at the Edge of AI Agents Software Engineer ML Engineer Data Scientist Engineering Leader Researcher Product Builder Mark your calendar May 13-14, 2025 The Midway San Francisco Your seat awaits Early bird tickets available now $499 Become a sponsor? Reach out to us at Got questions?',\n",
       " 'We\\'ll build a chatbot in LangGraph that can:  by searching the web  across calls  to a human for review  alternative conversation paths  we\\'ll start with a simple chatbot. Create a . Create an object defining the structure of our chatbot using LangGraph. Add a \" \" node. Add the graph\\'s schema and functions. Create our graph. Add checkpointing. Add human-in',\n",
       " 'Unresolved incident: Intermittent delays on run ingest in LangSmith US instance. No incidents reported or maintenance related to this downtime.',\n",
       " 'Build context-aware reasoning applications with LangChain’s flexible abstractions and AI-first toolkits. Core OSS libraries: Products: OSS extensions and apps:  Build context - a better UX for chat, writing content, and coding with LLMs. Build resilient language agents as graphs.',\n",
       " 'See if you can write your own integrations . Browse the LangChain integrations list to see if there are any providers you want to integrate with.',\n",
       " 'No summary available',\n",
       " 'No summary available',\n",
       " 'No summary available',\n",
       " \"Learn more about LangChain. Learn about LangGraph. Check out our reference section. Learn how to build applications with LLMs. See what's new in v0.3. Read about best practices.\",\n",
       " \"Learn about LangChain. Learn about how-to guides. Read up on how to build LLMs. Read about LangGraph. Read the developer's guide.\",\n",
       " 'No summary available',\n",
       " \"We explore how increasing the number of instructions and tools available to a single ReAct agent affects its performance, benchmarking models like claude-3.5-sonnet, gpt-4o, o1, and o3-mini across two domains of tasks. Vizient's GenAI platform helps users manage data to query for information ranging from patient outcomes to clinical benchmarking. See how they used LangGraph and LangSmith for multi-a\",\n",
       " \"Learn the basics of LangChain's LLM platform. Learn how to build agentic and multi-agent applications. Get started with LangChiain, LangSmith, and LangGraph.\",\n",
       " '. Observe your LLM application . Test and optimize your applications with high-quality evaluation data and metrics. Find the perfect prompt for your application.',\n",
       " 'Loading... ... .. .... .... ....... ......... ......................................................... ............ ........... .............. ..... .......................................... ................................. .......................................................... ............. ......................................',\n",
       " 'No summary available']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_lists=[]\n",
    "for g in lists:\n",
    "    if g not in summary_lists:\n",
    "        summary_lists.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A publicly-traded financial technology platform is helping large companies like Meta, Uber, H&M, and Microsoft achieve their ambitions faster by providing end-to-end payments capabilities, data-driven insights, and financial products in a single global solution. With more merchants signing on and with increased transaction volume comes increased pressure on support teams and a team at Adyen that immediately sought out leveraged solutions.',\n",
       " 'Elastic, a leading search analytics company, serving over 20k customers worldwide, enables organizations to securely harness search-powered AI so anyone can find the answers they need in real-time using all their data, at scale.',\n",
       " 'Ally Financial, the largest digital-only bank in the US and a leading auto lender, has recently collaborated with LangChain to release the first initial coding module that addresses a significant challenge for AI developers working with personal identifiable information (PII) in highly regulated, consumer-focused industries.',\n",
       " 'Create a new folder for the project. Run the code. Add your API keys. Add the constructor to your code. Run your code with the same command.',\n",
       " 'Exploring at the Edge of AI Agents Software Engineer ML Engineer Data Scientist Engineering Leader Researcher Product Builder Mark your calendar May 13-14, 2025 The Midway San Francisco Your seat awaits Early bird tickets available now $499 Become a sponsor? Reach out to us at Got questions?',\n",
       " 'We\\'ll build a chatbot in LangGraph that can:  by searching the web  across calls  to a human for review  alternative conversation paths  we\\'ll start with a simple chatbot. Create a . Create an object defining the structure of our chatbot using LangGraph. Add a \" \" node. Add the graph\\'s schema and functions. Create our graph. Add checkpointing. Add human-in',\n",
       " 'Unresolved incident: Intermittent delays on run ingest in LangSmith US instance. No incidents reported or maintenance related to this downtime.',\n",
       " 'Build context-aware reasoning applications with LangChain’s flexible abstractions and AI-first toolkits. Core OSS libraries: Products: OSS extensions and apps:  Build context - a better UX for chat, writing content, and coding with LLMs. Build resilient language agents as graphs.',\n",
       " 'See if you can write your own integrations . Browse the LangChain integrations list to see if there are any providers you want to integrate with.',\n",
       " 'No summary available',\n",
       " \"Learn more about LangChain. Learn about LangGraph. Check out our reference section. Learn how to build applications with LLMs. See what's new in v0.3. Read about best practices.\",\n",
       " \"Learn about LangChain. Learn about how-to guides. Read up on how to build LLMs. Read about LangGraph. Read the developer's guide.\",\n",
       " \"We explore how increasing the number of instructions and tools available to a single ReAct agent affects its performance, benchmarking models like claude-3.5-sonnet, gpt-4o, o1, and o3-mini across two domains of tasks. Vizient's GenAI platform helps users manage data to query for information ranging from patient outcomes to clinical benchmarking. See how they used LangGraph and LangSmith for multi-a\",\n",
       " \"Learn the basics of LangChain's LLM platform. Learn how to build agentic and multi-agent applications. Get started with LangChiain, LangSmith, and LangGraph.\",\n",
       " '. Observe your LLM application . Test and optimize your applications with high-quality evaluation data and metrics. Find the perfect prompt for your application.',\n",
       " 'Loading... ... .. .... .... ....... ......... ......................................................... ............ ........... .............. ..... .......................................... ................................. .......................................................... ............. ......................................']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame=pd.DataFrame(summary_lists,columns=['DOCUMENTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A publicly-traded financial technology platfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elastic, a leading search analytics company, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ally Financial, the largest digital-only bank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Create a new folder for the project. Run the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exploring at the Edge of AI Agents Software En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We'll build a chatbot in LangGraph that can:  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unresolved incident: Intermittent delays on ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Build context-aware reasoning applications wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>See if you can write your own integrations . B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No summary available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Learn more about LangChain. Learn about LangGr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Learn about LangChain. Learn about how-to guid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We explore how increasing the number of instru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Learn the basics of LangChain's LLM platform. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>. Observe your LLM application . Test and opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Loading... ... .. .... .... ....... ......... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            DOCUMENTS\n",
       "0   A publicly-traded financial technology platfor...\n",
       "1   Elastic, a leading search analytics company, s...\n",
       "2   Ally Financial, the largest digital-only bank ...\n",
       "3   Create a new folder for the project. Run the c...\n",
       "4   Exploring at the Edge of AI Agents Software En...\n",
       "5   We'll build a chatbot in LangGraph that can:  ...\n",
       "6   Unresolved incident: Intermittent delays on ru...\n",
       "7   Build context-aware reasoning applications wit...\n",
       "8   See if you can write your own integrations . B...\n",
       "9                                No summary available\n",
       "10  Learn more about LangChain. Learn about LangGr...\n",
       "11  Learn about LangChain. Learn about how-to guid...\n",
       "12  We explore how increasing the number of instru...\n",
       "13  Learn the basics of LangChain's LLM platform. ...\n",
       "14  . Observe your LLM application . Test and opti...\n",
       "15  Loading... ... .. .... .... ....... ......... ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['doc_ID']=data_frame.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame=data_frame[['doc_ID','DOCUMENTS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_ID</th>\n",
       "      <th>DOCUMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A publicly-traded financial technology platfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Elastic, a leading search analytics company, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ally Financial, the largest digital-only bank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Create a new folder for the project. Run the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Exploring at the Edge of AI Agents Software En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>We'll build a chatbot in LangGraph that can:  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Unresolved incident: Intermittent delays on ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Build context-aware reasoning applications wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>See if you can write your own integrations . B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>No summary available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Learn more about LangChain. Learn about LangGr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Learn about LangChain. Learn about how-to guid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>We explore how increasing the number of instru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Learn the basics of LangChain's LLM platform. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>. Observe your LLM application . Test and opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Loading... ... .. .... .... ....... ......... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_ID                                          DOCUMENTS\n",
       "0        1  A publicly-traded financial technology platfor...\n",
       "1        2  Elastic, a leading search analytics company, s...\n",
       "2        3  Ally Financial, the largest digital-only bank ...\n",
       "3        4  Create a new folder for the project. Run the c...\n",
       "4        5  Exploring at the Edge of AI Agents Software En...\n",
       "5        6  We'll build a chatbot in LangGraph that can:  ...\n",
       "6        7  Unresolved incident: Intermittent delays on ru...\n",
       "7        8  Build context-aware reasoning applications wit...\n",
       "8        9  See if you can write your own integrations . B...\n",
       "9       10                               No summary available\n",
       "10      11  Learn more about LangChain. Learn about LangGr...\n",
       "11      12  Learn about LangChain. Learn about how-to guid...\n",
       "12      13  We explore how increasing the number of instru...\n",
       "13      14  Learn the basics of LangChain's LLM platform. ...\n",
       "14      15  . Observe your LLM application . Test and opti...\n",
       "15      16  Loading... ... .. .... .... ....... ......... ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Embending In Fiass Index And Getting Similarity Distance Based On Query Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from faiss-cpu) (2.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (0.28.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashank_\\agenticai\\env2\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.6/13.7 MB 16.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.1/13.7 MB 18.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.1/13.7 MB 18.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.1/13.7 MB 18.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.0/13.7 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.8/13.7 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.0/13.7 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.1/13.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.1/13.7 MB 5.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.2/13.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.7/13.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.7/13.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.1/13.7 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 13:16:24 [sentence_transformers.SentenceTransformer] INFO: Use pytorch device_name: cpu\n",
      "2025-02-11 13:16:24 [sentence_transformers.SentenceTransformer] INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-11 13:16:24 [urllib3.connectionpool] DEBUG: Resetting dropped connection: huggingface.co\n",
      "2025-02-11 13:16:24 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:25 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:25 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:25 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:25 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:26 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "2025-02-11 13:16:26 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:26 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-02-11 13:16:27 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6800\n",
      "2025-02-11 13:16:27 [urllib3.connectionpool] DEBUG: https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6800\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert documents to embeddings\n",
    "document_embeddings = model.encode(data_frame['DOCUMENTS'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = document_embeddings.shape[1]\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in the FAISS index\n",
    "index.add(np.array(document_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, top_k=3):\n",
    "    # Convert query into embedding\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search the FAISS index for relevant documents\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    \n",
    "    # Return matching documents and corresponding IDs\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        doc_index = indices[0][i]  # Get index from FAISS results\n",
    "        results.append({\n",
    "            'ID': int(data_frame.iloc[doc_index]['doc_ID']),  # Use index to get the corresponding ID\n",
    "            'document': data_frame.iloc[doc_index]['DOCUMENTS'],  # Use index to get the document\n",
    "            'similarity_score': float(distances[0][i]) # Similarity score from FAISS\n",
    "        })\n",
    "    \n",
    "    return results,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_query('what is langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 9,\n",
       "  'document': 'See if you can write your own integrations . Browse the LangChain integrations list to see if there are any providers you want to integrate with.',\n",
       "  'similarity_score': 0.5289301872253418},\n",
       " {'ID': 14,\n",
       "  'document': \"Learn the basics of LangChain's LLM platform. Learn how to build agentic and multi-agent applications. Get started with LangChiain, LangSmith, and LangGraph.\",\n",
       "  'similarity_score': 0.43096399307250977},\n",
       " {'ID': 3,\n",
       "  'document': 'Ally Financial, the largest digital-only bank in the US and a leading auto lender, has recently collaborated with LangChain to release the first initial coding module that addresses a significant challenge for AI developers working with personal identifiable information (PII) in highly regulated, consumer-focused industries.',\n",
       "  'similarity_score': 0.3278048634529114}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Cosine Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Normalize embeddings before adding them to FAISS (cosine similarity)\n",
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "index = faiss.IndexFlatIP(dimension)  # IP = Inner Product (used for cosine similarity)\n",
    "index.add(np.array(document_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)  # Normalize query for cosine similarity\n",
    "\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        doc_index = indices[0][i]\n",
    "        results.append({\n",
    "            'ID': int(data_frame.iloc[doc_index]['doc_ID']),\n",
    "            'document': data_frame.iloc[doc_index]['DOCUMENTS'],\n",
    "            'similarity_score': float(distances[0][i])  # Now it's cosine similarity\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=search_query('what is langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 9,\n",
       "  'document': 'See if you can write your own integrations . Browse the LangChain integrations list to see if there are any providers you want to integrate with.',\n",
       "  'similarity_score': 0.5289301872253418},\n",
       " {'ID': 14,\n",
       "  'document': \"Learn the basics of LangChain's LLM platform. Learn how to build agentic and multi-agent applications. Get started with LangChiain, LangSmith, and LangGraph.\",\n",
       "  'similarity_score': 0.43096399307250977},\n",
       " {'ID': 3,\n",
       "  'document': 'Ally Financial, the largest digital-only bank in the US and a leading auto lender, has recently collaborated with LangChain to release the first initial coding module that addresses a significant challenge for AI developers working with personal identifiable information (PII) in highly regulated, consumer-focused industries.',\n",
       "  'similarity_score': 0.3278048634529114}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
